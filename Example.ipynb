{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surrogate Modeling Regression Example\n",
    "This example illustrates the difference between Kriging and the (Batch) XGBoost algorithm. \n",
    "The aim is to show the difference between Kriging and XGBoost on the exact same examples in\n",
    "the standard setting of regression. We do not expect a great difference in performance.\n",
    "Note that there is no iterative training in this example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Environment\n",
    "Note that this code has only been tested on Python 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Ignore Warnings \"\"\"\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the parameters and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the ABM Evaluation Budget\n",
    "budget = 50\n",
    "\n",
    "# Set out-of-sample test and montecarlo sizes\n",
    "test_size = 100\n",
    "montecarlos = 100\n",
    "\n",
    "# Get an on out-of-sample test set that does not have combinations from the\n",
    "# batch or iterative experiments\n",
    "final_test_size = (test_size * montecarlos)\n",
    "\n",
    "# Set the ABM parameters and support\n",
    "islands_exploration_range = np.array([\n",
    "    (0.0, 10),  # rho\n",
    "    (0.8, 2.0),  # alpha\n",
    "    (0.0, 1.0),  # phi\n",
    "    (0.0, 1.0),  # pi\n",
    "    (0.0, 1.0)])  # eps\n",
    "\n",
    "param_dims = islands_exploration_range.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the entire Budget in Batch\n",
    "Draw these samples according to a pseudo-random, well spaced generator. Here we use Sobol sequences because of their known coverage and compute performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amir/PycharmProjects/calib_ex/functions.py:236: RuntimeWarning: divide by zero encountered in log\n",
      "  log_GDP = np.log(GDP)\n"
     ]
    }
   ],
   "source": [
    "n_dimensions = islands_exploration_range.shape[0]\n",
    "evaluated_set_X_batch = get_sobol_samples(n_dimensions, budget, islands_exploration_range)\n",
    "evaluated_set_y_batch = evaluate_islands_on_set(evaluated_set_X_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate unique out-of-sample test set\n",
    "Note that this is taken uniformly at random from the support. If you take points that are structured by the sobol sequence generator, then the performance evaluation will not be blind, but biased. Given that the Sobol sequence is generated in an arguably linear fashion, this favors Kriging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oos_set = get_unirand_samples(n_dimensions, final_test_size*budget, islands_exploration_range)\n",
    "\n",
    "selections = []\n",
    "for i, v in enumerate(oos_set):\n",
    "    if (v not in evaluated_set_X_batch):\n",
    "        selections.append(i)\n",
    "oos_set = unique_rows(oos_set[selections])[:final_test_size]\n",
    "\n",
    "# Evaluate the test set for the ABM response\n",
    "y_test = evaluate_islands_on_set(oos_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Kriging Surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the default Gaussian Kernel and L-BFGS optimizer.\n",
    "surrogate_models_kriging = GaussianProcessRegressor(random_state=0)\n",
    "surrogate_models_kriging.fit(evaluated_set_X_batch, evaluated_set_y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the XGBoost Surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This surrogate will not have multiple iterations. It will run on the entire budget of evaluations.\n",
    "surrogate_model_XGBoost = fit_surrogate_model(evaluated_set_X_batch, evaluated_set_y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate both surrogates on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = [None] * 2\n",
    "mse_perf = np.zeros((2, montecarlos))\n",
    "\n",
    "y_hat_test[0] = surrogate_models_kriging.predict(oos_set)\n",
    "y_hat_test[1] = surrogate_model_XGBoost.predict(oos_set)\n",
    "\n",
    "# MSE performance\n",
    "for sur_idx in range(len(y_hat_test)):\n",
    "    for i in range(montecarlos):\n",
    "        mse_perf[sur_idx, i] = mean_squared_error(y_test[i * test_size:(i + 1) * test_size],\n",
    "                                                  y_hat_test[int(sur_idx)][i * test_size:(i + 1) * test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the out-of-sample Monte Carlo performance densities for each of the methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_labels = [\"Kriging\", \"XGBoost (Batch)\"]\n",
    "\n",
    "mse_perf = pd.DataFrame(mse_perf, index=experiment_labels)\n",
    "ax = plt.axes()\n",
    "sns.heatmap(mse_perf, ax=ax)\n",
    "ax.set_title('MSE Performance Heatmap over Monte-Carlos')\n",
    "ax.set_xlabel('')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "k_label = \"Kriging: Mean \" + str(mse_perf.iloc[0, :].mean()) + \", Variance \" + str(mse_perf.iloc[0, :].var())\n",
    "xgb_label = \"XGBoost: Mean \" + str(mse_perf.iloc[1, :].mean()) + \", Variance \" + str(mse_perf.iloc[1, :].var())\n",
    "\n",
    "fig1 = sns.distplot(mse_perf.iloc[0, :], label=k_label, ax=ax)\n",
    "fig2 = sns.distplot(mse_perf.iloc[1, :], label=xgb_label, ax=ax)\n",
    "\n",
    "plt.title(\"Out-Of-Sample Prediction Performance\")\n",
    "plt.xlabel('Mean-Squared Error')\n",
    "plt.yticks(fig1.get_yticks(), fig1.get_yticks() / 100)\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "fig.savefig(\"xgboost_kriging_ba_comparison_\" + str(budget) + \".png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
